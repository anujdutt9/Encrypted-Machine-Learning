{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning using Homomorphic Encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "import phe as paillier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we'll be using the Diabetes dataset from Scikit-Learn.\n",
    "\n",
    "The patients' data is split between 3 hospitals, USER-1, USER-2 and USER-3, all sharing the same features but different entities. We refer to this scenario as horizontally partitioned.\n",
    "\n",
    "The objective is to make use of the whole (virtual) training set to improve upon the model that can be trained locally at each hospital / USER location.\n",
    "\n",
    "AI Inc. make an initial model, trains it and sends to three different hospitals say USER-1, USER-2 and USER-3.\n",
    "\n",
    "None of the USERS know each other.\n",
    "\n",
    "So, every USER gets the initial model, trains it on it's own data and passes on the gradients to the other USER i.e.\n",
    "\n",
    "An additional agent is the 'server' who facilitates the information exchange\n",
    "among the hospitals under the following privacy constraints:\n",
    "1) The individual patient's record at each hospital cannot leave the premises, not even in encrypted form.\n",
    "2) Information derived (read: gradients) from any hospital's dataset cannot be shared, unless it is first encrypted.\n",
    "3) None of the parties (hospitals AND server) should be able to infer WHERE (in which hospital) a patient in the training set has been treated.\n",
    "\n",
    "### Working\n",
    "\n",
    "1. The server creates a paillier public/private keypair and does not share the private key.\n",
    "\n",
    "2. The hospital clients are given the public key. The protocol works as follows.\n",
    "\n",
    "3. Until convergence: hospital 1 computes its gradient, encrypts it and sends it to hospital 2; hospital 2 computes its gradient, encrypts and sums it to hospital 1's; hospital 3 does the same and passes the overall sum to the server. \n",
    "\n",
    "4. The server obtains the gradient of the whole (virtual) training set, decrypts it and sends the gradient back - in the clear - to every client.\n",
    "\n",
    "5. The clients then update their respective local models.\n",
    "\n",
    "From the security viewpoint, we consider all parties to be \"honest but curious\". Even by seeing the aggregated gradient in the clear, no participant can pinpoint where patients' data originated. This is true if this RING protocol is run by at least 3 clients, which prevents reconstruction of each others' gradients by simple difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Get the Data\n",
    "def get_data(n_clients):\n",
    "    \"\"\"\n",
    "    Import the dataset via sklearn, shuffle and split train/test.\n",
    "    Return training, target lists for `n_clients` and a holdout test set\n",
    "    \"\"\"\n",
    "    print(\"Loading data\")\n",
    "    diabetes = load_diabetes()\n",
    "    y = diabetes.target\n",
    "    X = diabetes.data\n",
    "\n",
    "    # Add constant to emulate intercept\n",
    "    X = np.c_[X, np.ones(X.shape[0])]\n",
    "\n",
    "    # The features are already preprocessed\n",
    "    # Shuffle\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    X, y = X[perm, :], y[perm]\n",
    "\n",
    "    # Select test at random\n",
    "    test_size = 50\n",
    "    test_idx = np.random.choice(X.shape[0], size=test_size, replace=False)\n",
    "    train_idx = np.ones(X.shape[0], dtype=bool)\n",
    "    train_idx[test_idx] = False\n",
    "    X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "    X_train, y_train = X[train_idx, :], y[train_idx]\n",
    "\n",
    "    # Split train among multiple clients.\n",
    "    # The selection is not at random. We simulate the fact that each client\n",
    "    # sees a potentially very different sample of patients.\n",
    "    X, y = [], []\n",
    "    step = int(X_train.shape[0] / n_clients)\n",
    "    for c in range(n_clients):\n",
    "        X.append(X_train[step * c: step * (c + 1), :])\n",
    "        y.append(y_train[step * c: step * (c + 1)])\n",
    "\n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE for Linear Regression\n",
    "def mean_square_error(y_pred, y):\n",
    "    \"\"\" 1/m * \\sum_{i=1..m} (y_pred_i - y_i)^2 \"\"\"\n",
    "    return np.mean((y - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to take in the Public Key and Encrypt the Data\n",
    "def encrypt_vector(public_key, x):\n",
    "    return [public_key.encrypt(i) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to take in the Private key and Decrypt the Data\n",
    "def decrypt_vector(private_key, x):\n",
    "    return np.array([private_key.decrypt(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Sum up the Encrypted Values\n",
    "def sum_encrypted_vectors(x, y):\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError('Encrypted vectors must have the same size')\n",
    "    return [x[i] + y[i] for i in range(len(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SERVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    \"\"\"Private key holder. Decrypts the average gradient\"\"\"\n",
    "\n",
    "    def __init__(self, key_length):\n",
    "        keypair = paillier.generate_paillier_keypair(n_length=key_length)\n",
    "        self.pubkey, self.privkey = keypair\n",
    "\n",
    "    def decrypt_aggregate(self, input_model, n_clients):\n",
    "        return decrypt_vector(self.privkey, input_model) / n_clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    \"\"\"Runs linear regression with local data or by gradient steps,\n",
    "    where gradient can be passed in.\n",
    "    Using public key can encrypt locally computed gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, X, y, pubkey):\n",
    "        self.name = name\n",
    "        self.pubkey = pubkey\n",
    "        self.X, self.y = X, y\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "\n",
    "    def fit(self, n_iter, eta=0.01):\n",
    "        \"\"\"Linear regression for n_iter\"\"\"\n",
    "        for _ in range(n_iter):\n",
    "            gradient = self.compute_gradient()\n",
    "            self.gradient_step(gradient, eta)\n",
    "\n",
    "    def gradient_step(self, gradient, eta=0.01):\n",
    "        \"\"\"Update the model with the given gradient\"\"\"\n",
    "        self.weights -= eta * gradient\n",
    "\n",
    "    def compute_gradient(self):\n",
    "        \"\"\"Compute the gradient of the current model using the training set\n",
    "        \"\"\"\n",
    "        delta = self.predict(self.X) - self.y\n",
    "        return delta.dot(self.X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Score test data\"\"\"\n",
    "        return X.dot(self.weights)\n",
    "\n",
    "    def encrypted_gradient(self, sum_to=None):\n",
    "        \"\"\"Compute and encrypt gradient.\n",
    "        When `sum_to` is given, sum the encrypted gradient to it, assumed\n",
    "        to be another vector of the same size\n",
    "        \"\"\"\n",
    "        gradient = self.compute_gradient()\n",
    "        encrypted_gradient = encrypt_vector(self.pubkey, gradient)\n",
    "\n",
    "        if sum_to is not None:\n",
    "            return sum_encrypted_vectors(sum_to, encrypted_gradient)\n",
    "        else:\n",
    "            return encrypted_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_learning(n_iter, eta, n_clients, key_length):\n",
    "    names = ['Hospital {}'.format(i) for i in range(1, n_clients + 1)]\n",
    "\n",
    "    X, y, X_test, y_test = get_data(n_clients=n_clients)\n",
    "\n",
    "    # Instantiate the server and generate private and public keys\n",
    "    # NOTE: using smaller keys sizes wouldn't be cryptographically safe\n",
    "    server = Server(key_length=key_length)\n",
    "\n",
    "    # Instantiate the clients.\n",
    "    # Each client gets the public key at creation and its own local dataset\n",
    "    clients = []\n",
    "    for i in range(n_clients):\n",
    "        clients.append(Client(names[i], X[i], y[i], server.pubkey))\n",
    "\n",
    "    # Each client trains a linear regressor on its own data\n",
    "    print('Error (MSE) that each client gets on test set by '\n",
    "          'training only on own local data:')\n",
    "    for c in clients:\n",
    "        c.fit(n_iter, eta)\n",
    "        y_pred = c.predict(X_test)\n",
    "        mse = mean_square_error(y_pred, y_test)\n",
    "        print('{:s}:\\t{:.2f}'.format(c.name, mse))\n",
    "\n",
    "    # The federated learning with gradient descent\n",
    "    print('Running distributed gradient aggregation for {:d} iterations'\n",
    "          .format(n_iter))\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        # Compute gradients, encrypt and aggregate\n",
    "        encrypt_aggr = clients[0].encrypted_gradient(sum_to=None)\n",
    "        for c in clients:\n",
    "            encrypt_aggr = c.encrypted_gradient(sum_to=encrypt_aggr)\n",
    "\n",
    "        # Send aggregate to server and decrypt it\n",
    "        aggr = server.decrypt_aggregate(encrypt_aggr, n_clients)\n",
    "\n",
    "        # Take gradient steps\n",
    "        for c in clients:\n",
    "            c.gradient_step(aggr, eta)\n",
    "\n",
    "    print('Error (MSE) that each client gets after running the protocol:')\n",
    "    for c in clients:\n",
    "        y_pred = c.predict(X_test)\n",
    "        mse = mean_square_error(y_pred, y_test)\n",
    "        print('{:s}:\\t{:.2f}'.format(c.name, mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Error (MSE) that each client gets on test set by training only on own local data:\n",
      "Hospital 1:\t3376.61\n",
      "Hospital 2:\t3347.62\n",
      "Hospital 3:\t3419.90\n",
      "Running distributed gradient aggregation for 50 iterations\n",
      "Error (MSE) that each client gets after running the protocol:\n",
      "Hospital 1:\t2794.57\n",
      "Hospital 2:\t2713.00\n",
      "Hospital 3:\t2846.74\n"
     ]
    }
   ],
   "source": [
    "federated_learning(n_iter=50, eta=0.01, n_clients=3, key_length=1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
